{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/owenmk/ok_cars/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRgTj35npSxs"
      },
      "source": [
        "# Cars Dataset\n",
        "Exploring the importance of size for supervised learning\n",
        "### reference\n",
        "[1] 3D Object Representations for Fine-Grained Categorization\n",
        "Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei\n",
        "4th IEEE Workshop on 3D Representation and Recognition, at ICCV 2013 (3dRR-13). Sydney, Australia. Dec. 8, 2013."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj6iS_3qpSxw"
      },
      "source": [
        "###  imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import torchvision.datasets.stanford_cars as stanford_cars\n",
        "import torchvision \n",
        "from torchvision import datasets, io, models, ops, transforms, utils\n",
        "from torch.nn import Identity\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import device\n",
        "from torch import save as torch_save\n",
        "from torch import load as torch_load\n",
        "\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# create logger\n",
        "logger = logging.getLogger(__name__)\n",
        "# set log level for all handlers to debug\n",
        "logger.setLevel(logging.DEBUG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>relative_im_path</th>\n",
              "      <th>bbox_x1</th>\n",
              "      <th>bbox_y1</th>\n",
              "      <th>bbox_x2</th>\n",
              "      <th>bbox_y2</th>\n",
              "      <th>class</th>\n",
              "      <th>test</th>\n",
              "      <th>class2</th>\n",
              "      <th>test2</th>\n",
              "      <th>relative_im_path2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[car_ims/000001.jpg]</td>\n",
              "      <td>[[112]]</td>\n",
              "      <td>[[7]]</td>\n",
              "      <td>[[853]]</td>\n",
              "      <td>[[717]]</td>\n",
              "      <td>[[1]]</td>\n",
              "      <td>[[0]]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>car_ims/000001.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[car_ims/000002.jpg]</td>\n",
              "      <td>[[48]]</td>\n",
              "      <td>[[24]]</td>\n",
              "      <td>[[441]]</td>\n",
              "      <td>[[202]]</td>\n",
              "      <td>[[1]]</td>\n",
              "      <td>[[0]]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>car_ims/000002.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[car_ims/000003.jpg]</td>\n",
              "      <td>[[7]]</td>\n",
              "      <td>[[4]]</td>\n",
              "      <td>[[277]]</td>\n",
              "      <td>[[180]]</td>\n",
              "      <td>[[1]]</td>\n",
              "      <td>[[0]]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>car_ims/000003.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[car_ims/000004.jpg]</td>\n",
              "      <td>[[33]]</td>\n",
              "      <td>[[50]]</td>\n",
              "      <td>[[197]]</td>\n",
              "      <td>[[150]]</td>\n",
              "      <td>[[1]]</td>\n",
              "      <td>[[0]]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>car_ims/000004.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[car_ims/000005.jpg]</td>\n",
              "      <td>[[5]]</td>\n",
              "      <td>[[8]]</td>\n",
              "      <td>[[83]]</td>\n",
              "      <td>[[58]]</td>\n",
              "      <td>[[1]]</td>\n",
              "      <td>[[0]]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>car_ims/000005.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16180</th>\n",
              "      <td>[car_ims/016181.jpg]</td>\n",
              "      <td>[[38]]</td>\n",
              "      <td>[[36]]</td>\n",
              "      <td>[[375]]</td>\n",
              "      <td>[[234]]</td>\n",
              "      <td>[[196]]</td>\n",
              "      <td>[[1]]</td>\n",
              "      <td>196</td>\n",
              "      <td>1</td>\n",
              "      <td>car_ims/016181.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16181</th>\n",
              "      <td>[car_ims/016182.jpg]</td>\n",
              "      <td>[[29]]</td>\n",
              "      <td>[[34]]</td>\n",
              "      <td>[[235]]</td>\n",
              "      <td>[[164]]</td>\n",
              "      <td>[[196]]</td>\n",
              "      <td>[[1]]</td>\n",
              "      <td>196</td>\n",
              "      <td>1</td>\n",
              "      <td>car_ims/016182.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16182</th>\n",
              "      <td>[car_ims/016183.jpg]</td>\n",
              "      <td>[[25]]</td>\n",
              "      <td>[[32]]</td>\n",
              "      <td>[[587]]</td>\n",
              "      <td>[[359]]</td>\n",
              "      <td>[[196]]</td>\n",
              "      <td>[[1]]</td>\n",
              "      <td>196</td>\n",
              "      <td>1</td>\n",
              "      <td>car_ims/016183.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16183</th>\n",
              "      <td>[car_ims/016184.jpg]</td>\n",
              "      <td>[[56]]</td>\n",
              "      <td>[[60]]</td>\n",
              "      <td>[[208]]</td>\n",
              "      <td>[[186]]</td>\n",
              "      <td>[[196]]</td>\n",
              "      <td>[[1]]</td>\n",
              "      <td>196</td>\n",
              "      <td>1</td>\n",
              "      <td>car_ims/016184.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16184</th>\n",
              "      <td>[car_ims/016185.jpg]</td>\n",
              "      <td>[[1]]</td>\n",
              "      <td>[[1]]</td>\n",
              "      <td>[[200]]</td>\n",
              "      <td>[[131]]</td>\n",
              "      <td>[[196]]</td>\n",
              "      <td>[[1]]</td>\n",
              "      <td>196</td>\n",
              "      <td>1</td>\n",
              "      <td>car_ims/016185.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16185 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           relative_im_path  bbox_x1 bbox_y1  bbox_x2  bbox_y2    class  \\\n",
              "0      [car_ims/000001.jpg]  [[112]]   [[7]]  [[853]]  [[717]]    [[1]]   \n",
              "1      [car_ims/000002.jpg]   [[48]]  [[24]]  [[441]]  [[202]]    [[1]]   \n",
              "2      [car_ims/000003.jpg]    [[7]]   [[4]]  [[277]]  [[180]]    [[1]]   \n",
              "3      [car_ims/000004.jpg]   [[33]]  [[50]]  [[197]]  [[150]]    [[1]]   \n",
              "4      [car_ims/000005.jpg]    [[5]]   [[8]]   [[83]]   [[58]]    [[1]]   \n",
              "...                     ...      ...     ...      ...      ...      ...   \n",
              "16180  [car_ims/016181.jpg]   [[38]]  [[36]]  [[375]]  [[234]]  [[196]]   \n",
              "16181  [car_ims/016182.jpg]   [[29]]  [[34]]  [[235]]  [[164]]  [[196]]   \n",
              "16182  [car_ims/016183.jpg]   [[25]]  [[32]]  [[587]]  [[359]]  [[196]]   \n",
              "16183  [car_ims/016184.jpg]   [[56]]  [[60]]  [[208]]  [[186]]  [[196]]   \n",
              "16184  [car_ims/016185.jpg]    [[1]]   [[1]]  [[200]]  [[131]]  [[196]]   \n",
              "\n",
              "        test  class2  test2   relative_im_path2  \n",
              "0      [[0]]       1      0  car_ims/000001.jpg  \n",
              "1      [[0]]       1      0  car_ims/000002.jpg  \n",
              "2      [[0]]       1      0  car_ims/000003.jpg  \n",
              "3      [[0]]       1      0  car_ims/000004.jpg  \n",
              "4      [[0]]       1      0  car_ims/000005.jpg  \n",
              "...      ...     ...    ...                 ...  \n",
              "16180  [[1]]     196      1  car_ims/016181.jpg  \n",
              "16181  [[1]]     196      1  car_ims/016182.jpg  \n",
              "16182  [[1]]     196      1  car_ims/016183.jpg  \n",
              "16183  [[1]]     196      1  car_ims/016184.jpg  \n",
              "16184  [[1]]     196      1  car_ims/016185.jpg  \n",
              "\n",
              "[16185 rows x 10 columns]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## The Cars Dataset can be found here: (https://ai.stanford.edu/~jkrause/cars/car_dataset.html). \n",
        "# Data from reference [1] has been copied to an accessible filepath.\n",
        "\n",
        "NUM_IMAGE = 16185\n",
        "\"\"\"The dataset contains 16185 images. Here numbered with idx = 0, 1, ...\"\"\"\n",
        "\n",
        "IMG_PATH = '/mnt/c/Users/Owen/Documents/img_data/'\n",
        "\"\"\"Data from reference [1] has been copied to an accessible filepath.\"\"\"\n",
        "\n",
        "# Make image data available in the notebook\n",
        "#\n",
        "# transform = transforms.Compose([transforms.Resize(255),\n",
        "#                                 transforms.CenterCrop(224),\n",
        "#                                 transforms.ToTensor()])\n",
        "transform = transforms.ToTensor()\n",
        "dataset = datasets.ImageFolder(IMG_PATH, transform=transform)\n",
        "\n",
        "# imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=True,num_workers=1)\n",
        "\n",
        "\n",
        "# Obtain train/test split and other annotations from the original author\n",
        "mat = scipy.io.loadmat(IMG_PATH +\"cars_annos.mat\")\n",
        "annotations = pd.DataFrame(mat['annotations'][0])\n",
        "annotations['class2']=[c[0,0] for c in annotations['class'].values] # manage indexing\n",
        "annotations['test2']=[c[0,0] for c in annotations['test'].values] # manage indexing\n",
        "annotations['relative_im_path2']=[c[0] for c in annotations['relative_im_path'].values] # manage indexing\n",
        "assert annotations.shape[0]==NUM_IMAGE, \"annotation problem\"\n",
        "annotations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Masking labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mask_labels(dataset_labels: List, proportion: float)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Remove a subset of labels from samples. Ensure each class has at least one labeled sample.\n",
        "    To remove 10% of labels, use proportion=0.1.\n",
        "    dataset_labels is a list of labels in sample order such that dataset_labels[0] is the label of the first sample and there are no gaps.\n",
        "    \"\"\"\n",
        "    # Cons\n",
        "    df = pd.DataFrame(data={'orig_label':dataset_label_list,'label':None})\n",
        "    df.index.name = 'idx'\n",
        "    num_samples = df.shape[0]\n",
        "\n",
        "    # Select samples to guarantee one from each class\n",
        "    keep_df = df.groupby('orig_label').apply(pd.DataFrame.sample, n=1)\n",
        "    keep_df = keep_df.reset_index('idx').reset_index(drop=True)\n",
        "    num_classes = keep_df.shape[0]\n",
        "\n",
        "    # How many samples to keep, in addition to 1 per class. Select them.\n",
        "    max_drop_frac = 1 - (num_classes / num_samples)\n",
        "    assert proportion <= max_drop_frac, f\"proportion must be less than {max_drop_frac}\"\n",
        "    num_new_keep = num_samples - num_classes - int(proportion*num_samples)\n",
        "    choose_from_idx = list(set(df.index) - set(keep_df['idx']))\n",
        "    new_keep_df = df.loc[choose_from_idx,:].sample(n=num_new_keep)\n",
        "\n",
        "    # For the ist of all sample indices to keep, transfer the known label\n",
        "    keep_idx = list(keep_df['idx']) + list(new_keep_df.index)\n",
        "    df.loc[keep_idx,'label'] = df.loc[keep_idx,'orig_label']\n",
        "    \n",
        "    return df\n",
        "# Example:\n",
        "dataset_label_list = annotations['class2'].tolist()\n",
        "proportion = 0.60 # 60% unlabelled\n",
        "label_df = mask_labels(dataset_label_list, proportion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data cleaning\n",
        "Find and delete any images that do not have 3 channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[48], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m num_deleted \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_IMAGE):\n\u001b[0;32m---> 22\u001b[0m     sample_img, _ \u001b[39m=\u001b[39m dataset[k]\n\u001b[1;32m     23\u001b[0m     \u001b[39m# num_chan = sample_img.size()[0]\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[39mif\u001b[39;00m (k\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimages checked: \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/ok_cars/lib/python3.9/site-packages/torchvision/datasets/folder.py:230\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[0;32m--> 230\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[1;32m    231\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
            "File \u001b[0;32m~/miniconda3/envs/ok_cars/lib/python3.9/site-packages/torchvision/datasets/folder.py:269\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    268\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
            "File \u001b[0;32m~/miniconda3/envs/ok_cars/lib/python3.9/site-packages/torchvision/datasets/folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpil_loader\u001b[39m(path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Image\u001b[39m.\u001b[39mImage:\n\u001b[1;32m    246\u001b[0m     \u001b[39m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m--> 248\u001b[0m         img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(f)\n\u001b[1;32m    249\u001b[0m         \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/ok_cars/lib/python3.9/site-packages/PIL/Image.py:3101\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3098\u001b[0m     fp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(fp\u001b[39m.\u001b[39mread())\n\u001b[1;32m   3099\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 3101\u001b[0m prefix \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(\u001b[39m16\u001b[39;49m)\n\u001b[1;32m   3103\u001b[0m preinit()\n\u001b[1;32m   3105\u001b[0m accept_warnings \u001b[39m=\u001b[39m []\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def is_3chan_image(tensor_size)->bool:\n",
        "    \"\"\"utility returns True for 3-channel images\"\"\"\n",
        "    # TODO: confirm functionality using synthesize bw image: transform = transforms.Grayscale()\n",
        "    num_chanels = tensor_size[0]\n",
        "    result = num_chanels == 3 \n",
        "    return result\n",
        "\n",
        "def delete_image(imgpath: str):\n",
        "    logger.info(f\"Deleting image {imgpath}\")\n",
        "    try:\n",
        "        # os.remove(imgpath)\n",
        "        logger.info(f\"Removing sample {imgpath}\")\n",
        "    except OSError as e:\n",
        "        logger.error(f\"Error {e}\")\n",
        "        raise\n",
        "\n",
        "# Scan the dataset to confirm all 3 channels are present in each image\n",
        "logger.info(\"Check all images have 3 channels...\")\n",
        "image_sizes = {}\n",
        "num_deleted = 0\n",
        "for k in range(NUM_IMAGE):\n",
        "    sample_img, _ = dataset[k]\n",
        "    # num_chan = sample_img.size()[0]\n",
        "    if (k+1) % 1000 == 0: logger.info(f\"images checked: {k+1}\")\n",
        "    sample_image_size = sample_img.size()\n",
        "    if not is_3chan_image(sample_image_size):\n",
        "        print(f\"non standard image {k}: size {sample_image_size}, _ {_}\")\n",
        "        img_path = IMG_PATH + annotations.loc[k,'relative_im_path2']\n",
        "        delete_image(img_path)\n",
        "        num_deleted += 1\n",
        "    image_sizes[k] = sample_image_size\n",
        "logger.info(f\"Number of images deleted / scanned: {num_deleted}/{NUM_IMAGE}\")\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset representation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torchvision.models.resnet18()\n",
        "model.fc = Identity()\n",
        "m = model.eval()\n",
        "\n",
        "NUM_SAMPLE_TEST = 20 # Reduce problem dimension for demonstration purposes\n",
        "dataset_rep = {k:{'embedding':None,'class_idx':annotations.loc[k,'class2'], 'labelled': True} for k in range(NUM_IMAGE)}\n",
        "for k in range(NUM_SAMPLE_TEST):\n",
        "    img, _ = dataset[k]\n",
        "    img = img[None,:,:,:] # batch dimension is needed allow using batch-api in a non-batch manner\n",
        "    embedding = m(img)\n",
        "    # dataset_rep[k] = embedding  # Uses too much memory for my machine\n",
        "    dataset_rep[k]['embedding'] = embedding[0,:10] # Reduce problem dimension for demonstration purposes\n",
        "    if (k+1) % 10 == 0: \n",
        "        save_path = IMG_PATH + f\"cars_emebeddings_{k+1}.pt\"\n",
        "        logger.info(f\"images embedded: {k+1}\")\n",
        "        torch_save(dataset_rep, save_path)\n",
        "save_path = IMG_PATH + f\"cars_emebeddings_{k+1}.pt\"\n",
        "logger.info(f\"Total number of images embedded: {k+1}\")\n",
        "torch_save(dataset_rep, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Partially labelled dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_IMAGES = 20 # revised down due to processing limitation\n",
        "load_path = IMG_PATH + f\"cars_emebeddings_{NUM_IMAGES}.pt\"\n",
        "dataset_1 = torch_load(load_path)\n",
        "new_dataset = pd.DataFrame(dataset_1).T\n",
        "\n",
        "dataset_label_list = annotations.loc[:NUM_IMAGES,'class2'].tolist()\n",
        "proportion = 0.60 # 60% unlabelled\n",
        "label_df = mask_labels(dataset_label_list, proportion)\n",
        "\n",
        "new_dataset = new_dataset[:NUM_IMAGES]\n",
        "new_dataset.index.name='idx'\n",
        "dataset_rep = new_dataset.copy()\n",
        "new_dataset.loc[:,'class_idx'] = label_df['label']\n",
        "new_dataset['labelled'] = new_dataset['class_idx'].notnull()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The amount of labeled data is shown "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False    11\n",
              "True      9\n",
              "Name: labelled, dtype: int64"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Count labeled and unlabeled samples in the new dataset\n",
        "new_dataset.labelled.value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train / validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def val_split(data_set_input :pd.DataFrame, data_set_labels:List, training_proportion:float):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data_set_input, data_set_labels, test_size= 1-training_proportion  )\n",
        "    return (X_train, X_test, y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(idx\n",
              " 11    [tensor(0.6834, grad_fn=<UnbindBackward0>), te...\n",
              " 19    [tensor(0.8647, grad_fn=<UnbindBackward0>), te...\n",
              " 14    [tensor(0.5153, grad_fn=<UnbindBackward0>), te...\n",
              " 4     [tensor(0.3922, grad_fn=<UnbindBackward0>), te...\n",
              " 15    [tensor(0.5952, grad_fn=<UnbindBackward0>), te...\n",
              " 7     [tensor(0.5582, grad_fn=<UnbindBackward0>), te...\n",
              " 12    [tensor(0.6606, grad_fn=<UnbindBackward0>), te...\n",
              " 10    [tensor(0.4729, grad_fn=<UnbindBackward0>), te...\n",
              " 8     [tensor(0.9126, grad_fn=<UnbindBackward0>), te...\n",
              " 2     [tensor(1.0437, grad_fn=<UnbindBackward0>), te...\n",
              " 3     [tensor(0.7866, grad_fn=<UnbindBackward0>), te...\n",
              " 1     [tensor(0.8416, grad_fn=<UnbindBackward0>), te...\n",
              " Name: embedding, dtype: object,\n",
              " idx\n",
              " 13    [tensor(0.6656, grad_fn=<UnbindBackward0>), te...\n",
              " 18    [tensor(0.6566, grad_fn=<UnbindBackward0>), te...\n",
              " 5     [tensor(0.5522, grad_fn=<UnbindBackward0>), te...\n",
              " 6     [tensor(0.8365, grad_fn=<UnbindBackward0>), te...\n",
              " 17    [tensor(1.0090, grad_fn=<UnbindBackward0>), te...\n",
              " 9     [tensor(0.6749, grad_fn=<UnbindBackward0>), te...\n",
              " 0     [tensor(1.0466, grad_fn=<UnbindBackward0>), te...\n",
              " 16    [tensor(0.6136, grad_fn=<UnbindBackward0>), te...\n",
              " Name: embedding, dtype: object,\n",
              " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=object),\n",
              " array([1, 1, 1, 1, 1, 1, 1, 1], dtype=object))"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# demonstrates the train/validations split\n",
        "data_set_input = dataset_rep['embedding']\n",
        "data_set_labels = dataset_rep['class_idx'].values\n",
        "training_proportion = 0.6\n",
        "X_train, X_test, y_train, y_test = val_split(data_set_input, data_set_labels, training_proportion)\n",
        "train_inputs, test_inputs, train_labels, test_labels = X_train, X_test, y_train, y_test\n",
        "train_inputs, test_inputs, train_labels, test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Active learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.15 ('ok_cars')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1d539626c73eb7a50bfddad0f9ffed477fdd81d467a59e945260fdd9461201d4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
